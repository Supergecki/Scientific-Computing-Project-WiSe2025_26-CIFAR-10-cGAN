model:
  latent_dim: 100  # dimension of the latent space
  embed_dim: 50  # dimension of the class label embedding
  generator_channels: [512, 256, 128, 64]
  discriminator_channels: [64, 128, 256, 512]

training:
  batch_size: 64  # I do not own a CUDA-compatible GPU, so setting this a bit lower
  num_epochs: 100  # might need to be regulated downwards? especially for testing purposes
  lr_g: 0.0002  # generator's learning rate
  lr_d: 0.0002  # discriminator's learning rate
  beta1: 0.5
  beta2: 0.999  # parameters for Adam optimizer

data:
  num_classes: 10
  image_size: 32  # these parameters are for working with CIFAR-10, they are incorrect
                  # in the example in the project description

seed: 42  # random seed, this choice is arbitrary but should stay the same for reproducibility
